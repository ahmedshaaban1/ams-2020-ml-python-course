{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Science Fundamentals\n",
    "\n",
    "In this module, we will examine dataset partitioning, covering cross-validation, feature selection and extraction, and other methods. In addition, we will explore parameter tuning for training optimal machine learning models. We will also introduce the python package used in the introductory course, scikit-learn, and discuss linear models. You will train a linear model on the NCAR dataset introduced in the previous module.\n",
    "\n",
    "**Please cite the notebook as follows:**\n",
    "\n",
    "**insert names here**???, 2020: \"Data Science Fundamentals: Python tutorial\". **insert url here**???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## References\n",
    "\n",
    "This notebook refers to a few publications, listed below.\n",
    "\n",
    "Chisholm, D., J. Ball, K. Veigas, and P. Luty, 1968: \"The diagnosis of upper-level humidity.\" *Journal of Applied Meteorology*, **7 (4)**, 613-619.\n",
    "\n",
    "Hsu, W., and A. Murphy, 1986: \"The attributes diagram: A geometrical framework for assessing the quality of probability forecasts.\" *International Journal of Forecasting*, **2**, 285–293, https://doi.org/10.1016/0169-2070(86)90048-8.\n",
    "\n",
    "McGovern, A., D. Gagne II, J. Basara, T. Hamill, and D. Margolin, 2015: \"Solar energy prediction: An international contest to initiate interdisciplinary research on compelling meteorological problems.\" *Bulletin of the American Meteorological Society*, **96 (8)**, 1388-1395.\n",
    "\n",
    "Metz, C., 1978: \"Basic principles of ROC analysis.\" *Seminars in Nuclear Medicine*, **8**, 283–298, https://doi.org/10.1016/S0001-2998(78)80014-2.\n",
    "\n",
    "Quinlan, J., 1986: \"Induction of decision trees.\" *Machine Learning*, **1 (1)**, 81–106.\n",
    "\n",
    "Roebber, P., 2009: \"Visualizing multiple measures of forecast quality.\" *Weather and Forecasting*, **24**, 601-608, https://doi.org/10.1175/2008WAF2222159.1.\n",
    "\n",
    "Schwartz, C., G. Romine, M. Weisman, R. Sobash, K. Fossell, K. Manning, and S. Trier, 2015: \"A real-time convection-allowing ensemble prediction system initialized by mesoscale ensemble Kalman filter analyses.\" *Weather and Forecasting*, **30 (5)**, 1158-1181."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup\n",
    "\n",
    "To use this notebook, you will need Python 3.6 and the following packages.\n",
    "\n",
    " - scipy\n",
    " - TensorFlow\n",
    " - Keras\n",
    " - scikit-image\n",
    " - netCDF4\n",
    " - pyproj\n",
    " - scikit-learn\n",
    " - opencv-python\n",
    " - matplotlib\n",
    " - shapely\n",
    " - geopy\n",
    " - metpy\n",
    " - descartes\n",
    "\n",
    "If you have Anaconda on a Linux or Mac, you can install these packages with the commands `pip install scipy`, `pip install tensorflow`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "The next cell imports all libraries that will be used by this notebook.  If the notebook crashes anywhere, it will probably be here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "from os.path import dirname, abspath\n",
    "sys.path.insert(1, abspath('')+\"/Introduction_to_Machine_Learning\")\n",
    "\n",
    "import copy\n",
    "import warnings\n",
    "import numpy\n",
    "import matplotlib.pyplot as pyplot\n",
    "import Data_Science_Fundamentals.utils as utils\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEPARATOR_STRING = '\\n\\n' + '*' * 50 + '\\n\\n'\n",
    "MINOR_SEPARATOR_STRING = '\\n\\n' + '-' * 50 + '\\n\\n'\n",
    "\n",
    "MODULE2_DIR_NAME = '.'\n",
    "SHORT_COURSE_DIR_NAME = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}\n",
      "text/plain": "<IPython.core.display.Javascript object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset Partitioning\n",
    "\n",
    "### Training Data\n",
    "Train the model (*e.g.*, adjust weights in linear regression or neural net, adjust decision boundaries in decision tree).\n",
    "<br>\n",
    "### Validation Data\n",
    "Choose the best hyperparameters (*e.g.*, learning rate, number of iterations, number of layers in a neural net).\n",
    "\n",
    "*Note:* A **parameter** is a ???, while a **hyperparameter** is a ???.\n",
    "<br>\n",
    "### Testing Data \n",
    "Evaluate the \"best\" model on independent data.\n",
    "\n",
    "The \"best\" model is the one that performs best on validation data. There are many ways to define the best performance (*e.g.*, lowest mean squared error, lowest cross-entropy, highest area under ROC curve, etc.).\n",
    "<br><br>\n",
    "_____\n",
    "\n",
    "### Statistical Independence \n",
    "\n",
    "Training, validation, and testing (TV&T) sets should be **statistically independent**.\n",
    "<br><br>\n",
    "For example, \n",
    " - **In disease prediction:**\n",
    "     - If there are multiple tissue scans from the same patient, they should all be in the same set.\n",
    "     - Multiple patients in the same family should also be in the same set.\n",
    "<br><br>\n",
    " - **In storm-hazard prediction:**\n",
    "     - If there are multiple radar scans from the same storm, they should all be in the same set.\n",
    "     - Related storms (*e.g.*, part of the same MCS) should also be in the same set.\n",
    "<br><br>\n",
    " - **In weather prediction generally:**\n",
    "     - Data should be free of temporal autocorrelation.\n",
    "     - For storm-scale phenomena, it is probably sufficient to leave a one-day gap between each pair of datasets.\n",
    "     - For synoptic-scale phenomena, you may need a one-week gap between each pair of datasets.<br><br>\n",
    "     - If training on one area and applying to a different area, data should be free of spatial autocorrelation.\n",
    "     - For example, say you are training on a model on North America but applying in real-time to Africa.\n",
    "     - TV&T sets should contain spatially non-overlapping parts of North America.\n",
    "     - When you test on a part of North America that has not been used to train/validate the model, hopefully this gives you a reasonable estimation of performance in Africa, which also has not been used to train/validate the model.\n",
    "\n",
    "**Question:** Our NCAR dataset contains data from the year 2010 to the year 2017. Which data should be in which set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find Input Files\n",
    "\n",
    "The next cell finds input files for the training (2010-14), validation (2015), and testing (2016-17) periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_names = utils.find_many_feature_files(\n",
    "    first_date_string='20100101', last_date_string='20141231')\n",
    "\n",
    "validation_file_names = utils.find_many_feature_files(\n",
    "    first_date_string='20150101', last_date_string='20151231')\n",
    "\n",
    "testing_file_names = utils.find_many_feature_files(\n",
    "    first_date_string='20160101', last_date_string='20171231')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Data\n",
    "\n",
    "The next cell reads training, validation, and testing data and explores the contents of one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a98c0fbfb00c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m (training_metadata_table, training_predictor_table_denorm,\n\u001b[1;32m      2\u001b[0m  \u001b[0mtraining_target_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m ) = utils.read_many_feature_files(training_file_names)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMINOR_SEPARATOR_STRING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/ams-2020-ml-python-course/Introduction_to_Machine_Learning/Data_Science_Fundamentals/utils.py\u001b[0m in \u001b[0;36mread_many_feature_files\u001b[0;34m(csv_file_names)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     metadata_table = pandas.concat(\n\u001b[0;32m--> 244\u001b[0;31m         list_of_metadata_tables, axis=0, ignore_index=True)\n\u001b[0m\u001b[1;32m    245\u001b[0m     predictor_table = pandas.concat(\n\u001b[1;32m    246\u001b[0m         list_of_predictor_tables, axis=0, ignore_index=True)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    204\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                        copy=copy)\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No objects to concatenate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "(training_metadata_table, training_predictor_table_denorm,\n",
    " training_target_table\n",
    ") = utils.read_many_feature_files(training_file_names)\n",
    "print(MINOR_SEPARATOR_STRING)\n",
    "\n",
    "(validation_metadata_table, validation_predictor_table_denorm,\n",
    " validation_target_table\n",
    ") = utils.read_many_feature_files(validation_file_names)\n",
    "print(MINOR_SEPARATOR_STRING)\n",
    "\n",
    "(testing_metadata_table, testing_predictor_table_denorm,\n",
    " testing_target_table\n",
    ") = utils.read_many_feature_files(testing_file_names)\n",
    "print(MINOR_SEPARATOR_STRING)\n",
    "\n",
    "print('Variables in metadata are as follows:\\n{0:s}'.format(\n",
    "    str(list(training_metadata_table))\n",
    "))\n",
    "\n",
    "print('\\nPredictor variables are as follows:\\n{0:s}'.format(\n",
    "    str(list(training_predictor_table_denorm))\n",
    "))\n",
    "\n",
    "print('\\nTarget variable is as follows:\\n{0:s}'.format(\n",
    "    str(list(training_target_table))\n",
    "))\n",
    "\n",
    "first_predictor_name = list(training_predictor_table_denorm)[0]\n",
    "these_predictor_values = (\n",
    "    training_predictor_table_denorm[first_predictor_name].values[:10]\n",
    ")\n",
    "\n",
    "message_string = (\n",
    "    '\\nValues of predictor variable \"{0:s}\" for the first training '\n",
    "    'examples:\\n{1:s}'\n",
    ").format(first_predictor_name, str(these_predictor_values))\n",
    "print(message_string)\n",
    "\n",
    "target_name = list(training_target_table)[0]\n",
    "these_target_values = training_target_table[target_name].values[:10]\n",
    "\n",
    "message_string = (\n",
    "    '\\nValues of target variable for the first training examples:\\n{0:s}'\n",
    ").format(str(these_target_values))\n",
    "print(message_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting\n",
    "\n",
    "You can't talk about machine learning without talking about overfitting. Overfitting occurs when the model performs well on training data but does not generalize well to new data.\n",
    "<br><br>\n",
    "Overfitting usually occurs in the following scenarios:\n",
    " - **Training set is small**.\n",
    " - Training set includes **inappropriate predictors**.\n",
    " - Training set is **not representative of the real world**.\n",
    "<br><br>\n",
    " **Examples of inappropriate predictors**:\n",
    " - Using storm ID to predict tornadoes.\n",
    " - Using patient ID to predict disease.\n",
    " - In general, using variables with no physical relationship to the target phenomenon.\n",
    "<br><br>\n",
    " **Examples of non-representative training data**:\n",
    " - Different label distributions (*e.g.*, training set contains 50% tornadic storms, but in the real world $<$ 1% of storms are tornadic).\n",
    " - Different levels of data quality (*e.g.*, training on archived radar data but applying to real-time data).\n",
    "<br><br>\n",
    "\n",
    "You can **mitigate** overfitting by not making these mistakes, but these properties (appropriateness of predictor, representativity of training data, etc.) are not always clear. Many other issues can also lead to overfitting, so you also need the ability to **diagnose** overfitting. This is usually done by splitting data into 3 partitions: training, validation, and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "\n",
    " - **When you have multiple predictor variables on different scales, they should be normalized.**\n",
    " - This ensures that the model does not ignore variables with smaller scales.\n",
    " - For example, if a model is trained with temperature in Kelvins and specific humidity in kg kg$^{-1}$, it will probably learn to emphasize temperature (which varies from $\\sim$180-330 K) and ignore specific humidity (which varies from $\\sim$0-0.02 kg kg$^{-1}$).\n",
    "<br><br>\n",
    " - **The most common normalization method is $z$-scores.**\n",
    " - Each predictor variable is transformed independently to $z$-scores, using the mean and standard deviation from the training data.\n",
    " - Validation and testing data should also be normalized, but using the means and standard deviations from the training data.\n",
    "<br><br>\n",
    " - **Question:** why is it a bad idea to use the validation/testing data to compute means and standard deviations (or any other data-processing parameters)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization Code\n",
    "\n",
    "The next cell does the following:\n",
    "\n",
    " - Finds normalization parameters (mean and standard deviation) for each predictor, based only on the training data.\n",
    " - Normalizes the training, validation, and testing data, using these normalization params.\n",
    " - Denormalizes the training data and ensures that denormalized values = original values (sanity check)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_names = list(training_predictor_table_denorm)\n",
    "these_predictor_values = (\n",
    "    training_predictor_table_denorm[predictor_names[0]].values[:10]\n",
    ")\n",
    "\n",
    "message_string = (\n",
    "    'Original values of \"{0:s}\" for the first training examples:\\n{1:s}'\n",
    ").format(predictor_names[0], str(these_predictor_values))\n",
    "print(message_string)\n",
    "\n",
    "training_predictor_table, normalization_dict = utils.normalize_predictors(\n",
    "    predictor_table=copy.deepcopy(training_predictor_table_denorm)\n",
    ")\n",
    "\n",
    "these_predictor_values = (\n",
    "    training_predictor_table[predictor_names[0]].values[:10]\n",
    ")\n",
    "\n",
    "message_string = (\n",
    "    '\\nNormalized values of \"{0:s}\" for the first training examples:\\n{1:s}'\n",
    ").format(predictor_names[0], str(these_predictor_values))\n",
    "print(message_string)\n",
    "\n",
    "training_predictor_table_denorm = utils.denormalize_predictors(\n",
    "    predictor_table=copy.deepcopy(training_predictor_table),\n",
    "    normalization_dict=normalization_dict\n",
    ")\n",
    "\n",
    "these_predictor_values = (\n",
    "    training_predictor_table_denorm[predictor_names[0]].values[:10]\n",
    ")\n",
    "\n",
    "message_string = (\n",
    "    '\\n*De*normalized values (should equal original values) of \"{0:s}\" for '\n",
    "    'the first training examples:\\n{1:s}'\n",
    ").format(predictor_names[0], str(these_predictor_values))\n",
    "print(message_string)\n",
    "\n",
    "validation_predictor_table, _ = utils.normalize_predictors(\n",
    "    predictor_table=copy.deepcopy(validation_predictor_table_denorm),\n",
    "    normalization_dict=normalization_dict)\n",
    "\n",
    "testing_predictor_table, _ = utils.normalize_predictors(\n",
    "    predictor_table=copy.deepcopy(testing_predictor_table_denorm),\n",
    "    normalization_dict=normalization_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "\n",
    "**Linear regression fits the following equation to the training data.**\n",
    "\n",
    "$\\hat{y} = \\beta_0 + \\sum\\limits_{j = 1}^{M} \\beta_j x_j$\n",
    "\n",
    " - $x_j$ = $j^{\\textrm{th}}$ predictor variable\n",
    " - $\\beta_j$ = coefficient for $j^{\\textrm{th}}$ predictor variable (adjusted during training)\n",
    " - $M$ = number of predictor variables\n",
    " - $\\beta_0$ = bias coefficient or \"intercept\" (adjusted during training)\n",
    " - $\\hat{y}$ = prediction for target variable (in this case, max future vorticity within storm, in s$^{-1}$)\n",
    "\n",
    "<br>\n",
    "**The weights ($\\beta_0$ and $\\beta_j$) are trained to minimize the mean squared error (MSE).**  This is why linear regression is often called \"least-squares linear regression\".\n",
    "\n",
    "$\\textrm{MSE} = \\frac{1}{N} \\sum\\limits_{i = 1}^{N} (\\hat{y}_i - y_i)^2$\n",
    "\n",
    " - $y_i$ = actual target value for $i^{\\textrm{th}}$ example (in this case, one example is one storm cell at one time, or one \"storm object\")\n",
    " - $\\hat{y}_i$ = predicted target value for $i^{\\textrm{th}}$ example\n",
    " - $N$ = number of training examples\n",
    "\n",
    "Combining the two equations yields the following, where $x_{ij}$ is the $j^{\\textrm{th}}$ predictor for the $i^{\\textrm{th}}$ example.\n",
    "\n",
    "$\\textrm{MSE} = \\frac{1}{N} \\sum\\limits_{i = 1}^{N} (\\beta_0 + \\sum\\limits_{j = 1}^{M} \\beta_j x_{ij} - y_i)^2$\n",
    "\n",
    "The derivatives of model coefficients with respect to MSE are as follows.\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\beta_0}(\\textrm{MSE}) = \\frac{2}{N} \\sum\\limits_{i = 1}^{N} (\\hat{y}_i - y_i)$\n",
    "<br>\n",
    "$\\frac{\\partial}{\\partial \\beta_j}(\\textrm{MSE}) = \\frac{2}{N} \\sum\\limits_{i = 1}^{N} x_{ij} (\\hat{y}_i - y_i)$\n",
    "\n",
    "**During training, the weights ($\\beta_0$ and $\\beta_j$) are adjusted over many iterations.**  After each iteration, the \"gradient-descent rule\" (shown below) is applied, where $\\alpha \\in \\left(0, 1\\right]$ is the learning rate.\n",
    "\n",
    "$\\beta_0 \\leftarrow \\beta_0 - \\alpha \\frac{\\partial}{\\partial \\beta_0}(\\textrm{MSE})$\n",
    "<br>\n",
    "$\\beta_j \\leftarrow \\beta_j - \\alpha \\frac{\\partial}{\\partial \\beta_j}(\\textrm{MSE})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression: Example\n",
    "\n",
    "**The next cell does the following:**\n",
    "\n",
    " - Trains a linear-regression model (with default hyperparameters) to predict max future rotation in each storm.\n",
    " - Evaluates the model on both training and validation data.\n",
    "\n",
    "**For both training and validation data, this cell reports the following quantities:**\n",
    "\n",
    " - Mean absolute error (MAE): $\\frac{1}{N} \\sum\\limits_{i = 1}^{N} \\lvert \\hat{y}_i - y_i \\rvert$\n",
    " - Mean squared error (MSE): $\\frac{1}{N} \\sum\\limits_{i = 1}^{N} (\\hat{y}_i - y_i)^2$\n",
    " - Mean signed error (\"bias\"): $\\frac{1}{N} \\sum\\limits_{i = 1}^{N} (\\hat{y}_i - y_i)$\n",
    " - MAE skill score.  This is defined as follows, where MAE is the MAE of the model and $\\textrm{MAE}_{\\textrm{climo}}$ is the MAE obtained by always predicting \"climatology\" (the average in the training data).\n",
    "\n",
    "$\\textrm{MAE skill score} = \\frac{\\textrm{MAE}_{\\textrm{climo}} - \\textrm{MAE}}{\\textrm{MAE}_{\\textrm{climo}}}$\n",
    "\n",
    " - MSE skill score, defined as follows.\n",
    "\n",
    "$\\textrm{MSE skill score} = \\frac{\\textrm{MSE}_{\\textrm{climo}} - \\textrm{MSE}}{\\textrm{MSE}_{\\textrm{climo}}}$\n",
    "\n",
    "Finally, this cell plots a **reliability curve**, which shows the conditional mean observation for each forecast value.  This allows you to identify conditional bias (bias that occurs for certain forecast values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_model_object = utils.setup_linear_regression(\n",
    "    lambda1=0., lambda2=0.)\n",
    "\n",
    "_ = utils.train_linear_regression(\n",
    "    model_object=linreg_model_object,\n",
    "    training_predictor_table=training_predictor_table,\n",
    "    training_target_table=training_target_table)\n",
    "\n",
    "training_predictions = linreg_model_object.predict(\n",
    "    training_predictor_table.as_matrix()\n",
    ")\n",
    "mean_training_target_value = numpy.mean(\n",
    "    training_target_table[utils.TARGET_NAME].values\n",
    ")\n",
    "\n",
    "_ = utils.evaluate_regression(\n",
    "    target_values=training_target_table[utils.TARGET_NAME].values,\n",
    "    predicted_target_values=training_predictions,\n",
    "    mean_training_target_value=mean_training_target_value,\n",
    "    dataset_name='training')\n",
    "print(MINOR_SEPARATOR_STRING)\n",
    "\n",
    "validation_predictions = linreg_model_object.predict(\n",
    "    validation_predictor_table.as_matrix()\n",
    ")\n",
    "\n",
    "_ = utils.evaluate_regression(\n",
    "    target_values=validation_target_table[utils.TARGET_NAME].values,\n",
    "    predicted_target_values=validation_predictions,\n",
    "    mean_training_target_value=mean_training_target_value,\n",
    "    dataset_name='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression: Coefficients\n",
    "\n",
    "The next cell plots coefficients for the linear-regression model.  If predictor $x_j$ has a positive (negative) coefficient, the prediction increases (decreases) with $x_j$.\n",
    "<br><br>\n",
    "Keep in mind that all predictors have been normalized to the same scale ($z$-scores), so **generally** predictors with larger coefficients are more important.\n",
    "<br><br>\n",
    "Also, note that every predictor is used (has a non-zero coefficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_model_coefficients(\n",
    "    model_object=linreg_model_object,\n",
    "    predictor_names=list(training_predictor_table)\n",
    ")\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# $L_1$ and $L_2$ Regularization\n",
    "\n",
    " - In general, **regularization is a way to prevent overfitting by creating a simpler model.**\n",
    " - $L_1$ and $L_2$ regularization encourage the model to have smaller coefficients.\n",
    "<br><br>\n",
    " - **This is useful when there are many predictors**, because it is likely that some of these predictors have a weak causal relationship with the phenomenon.\n",
    " - Without regularization the model will try to incorporate every predictor, which may lead to overfitting peculiarities of the training data.\n",
    " - Regularization encourages the model to learn large coefficients for only a small number of predictors (the **really** important ones).\n",
    "<br><br>\n",
    " - **$L_1$ and $L_2$ regularization encourage smaller coefficients by adding penalties to the loss function.**\n",
    " - For linear regression, the loss function turns into the following.\n",
    "\n",
    "$\\epsilon = \\frac{1}{N} \\sum\\limits_{i = 1}^{N} (\\hat{y}_i - y_i)^2 + \\lambda_1 \\sum\\limits_{j = 1}^{M} \\lvert \\beta_j \\rvert + \\lambda_2 \\sum\\limits_{j = 1}^{M} \\beta_j^2 = \\textrm{MSE} + \\lambda_1 \\sum\\limits_{j = 1}^{M} \\lvert \\beta_j \\rvert + \\lambda_2 \\sum\\limits_{j = 1}^{M} \\beta_j^2$\n",
    "\n",
    " - The first term is just MSE (mean squared error).\n",
    " - The second term is the $L_1$ penalty.  $\\lambda_1$ is the strength of the $L_1$ penalty, and $\\sum\\limits_{j = 1}^{M} \\lvert \\beta_j \\rvert$ is the sum of absolute coefficient values.\n",
    " - The third term is the $L_2$ penalty.  $\\lambda_2$ is the strength of the $L_2$ penalty, and $\\lambda_2 \\sum\\limits_{j = 1}^{M} \\beta_j^2$ is the sum of squared coefficient values.\n",
    "<br><br>\n",
    " - Both penalties encourage smaller coefficients, but the $L_1$ penalty also encourages fewer non-zero coefficients.\n",
    " - This is because the $L_1$ penalty does not square coefficient values.\n",
    " - For small coefficient values the $L_2$ penalty becomes negligible unless $L_2$ is very large.  For example, squaring a coefficient of $10^{-3}$ yields $10^{-6}$, and this penalty is usually negligible.\n",
    "<br><br>\n",
    " - For this reason the $L_1$ penalty is called the **\"lasso penalty\"** (it throws a lasso and keeps only the predictors inside the lasso).\n",
    " - The $L_2$ penalty is called the **\"ridge penalty\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Linear Regression with $L_2$: Example\n",
    "\n",
    "The next cell trains a linear-regression model with only the $L_2$ penalty.  The regularization strength ($\\lambda_2$) is $10^{5}$.\n",
    "<br><br>\n",
    "Note that both training and validation performance get worse.  This means that the $\\lambda_2$ value attempted is too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_ridge_model_object = utils.setup_linear_regression(\n",
    "    lambda1=0., lambda2=1e5)\n",
    "\n",
    "_ = utils.train_linear_regression(\n",
    "    model_object=linear_ridge_model_object,\n",
    "    training_predictor_table=training_predictor_table,\n",
    "    training_target_table=training_target_table)\n",
    "\n",
    "training_predictions = linear_ridge_model_object.predict(\n",
    "    training_predictor_table.as_matrix()\n",
    ")\n",
    "mean_training_target_value = numpy.mean(\n",
    "    training_target_table[utils.TARGET_NAME].values\n",
    ")\n",
    "\n",
    "_ = utils.evaluate_regression(\n",
    "    target_values=training_target_table[utils.TARGET_NAME].values,\n",
    "    predicted_target_values=training_predictions,\n",
    "    mean_training_target_value=mean_training_target_value,\n",
    "    dataset_name='training')\n",
    "print(MINOR_SEPARATOR_STRING)\n",
    "\n",
    "validation_predictions = linear_ridge_model_object.predict(\n",
    "    validation_predictor_table.as_matrix()\n",
    ")\n",
    "\n",
    "_ = utils.evaluate_regression(\n",
    "    target_values=validation_target_table[utils.TARGET_NAME].values,\n",
    "    predicted_target_values=validation_predictions,\n",
    "    mean_training_target_value=mean_training_target_value,\n",
    "    dataset_name='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression with $L_2$: Coefficients\n",
    "\n",
    "The next cell plots coefficients for linear regression with only the $L_2$ penalty.\n",
    "<br><br>\n",
    "Note that coefficients are generally an order of magnitude smaller than in the original model ($10^{-5}$ to $10^{-4}$, instead of $10^{-4}$ to $10^{-3}$).\n",
    "<br><br>\n",
    "However, all coefficients are non-zero, because as discussed the $L_2$ penalty does not encourage coefficients to become **exactly** zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_model_coefficients(\n",
    "    model_object=linear_ridge_model_object,\n",
    "    predictor_names=list(training_predictor_table)\n",
    ")\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression with $L_1$: Example\n",
    "\n",
    "The next cell trains a linear-regression model with only the $L_1$ penalty.  The regularization strength ($\\lambda_1$) is $10^{-5}$.\n",
    "<br><br>\n",
    "Both training and validation performance get **a bit** worse than in the original model, which means that the $\\lambda_1$ value is too high.\n",
    "<br><br>\n",
    "However, performance does not decrease as much as it did for $L_2$ regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_lasso_model_object = utils.setup_linear_regression(\n",
    "    lambda1=1e-5, lambda2=0.)\n",
    "\n",
    "_ = utils.train_linear_regression(\n",
    "    model_object=linear_lasso_model_object,\n",
    "    training_predictor_table=training_predictor_table,\n",
    "    training_target_table=training_target_table)\n",
    "\n",
    "training_predictions = linear_lasso_model_object.predict(\n",
    "    training_predictor_table.as_matrix()\n",
    ")\n",
    "mean_training_target_value = numpy.mean(\n",
    "    training_target_table[utils.TARGET_NAME].values\n",
    ")\n",
    "\n",
    "_ = utils.evaluate_regression(\n",
    "    target_values=training_target_table[utils.TARGET_NAME].values,\n",
    "    predicted_target_values=training_predictions,\n",
    "    mean_training_target_value=mean_training_target_value,\n",
    "    dataset_name='training')\n",
    "print(MINOR_SEPARATOR_STRING)\n",
    "\n",
    "validation_predictions = linear_lasso_model_object.predict(\n",
    "    validation_predictor_table.as_matrix()\n",
    ")\n",
    "\n",
    "_ = utils.evaluate_regression(\n",
    "    target_values=validation_target_table[utils.TARGET_NAME].values,\n",
    "    predicted_target_values=validation_predictions,\n",
    "    mean_training_target_value=mean_training_target_value,\n",
    "    dataset_name='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression with $L_1$: Coefficients\n",
    "\n",
    "The next cell plots coefficients for linear regression with only the $L_1$ penalty.\n",
    "<br><br>\n",
    "Non-zero coefficients are generally on the same order of magnitude as the original model ($\\sim10^{-4}$).\n",
    "<br><br>\n",
    "However, many coefficients (25 of 41) have been \"zeroed out\".  This means that the model uses only 16 of the 41 predictors.  (Note: This result may vary slightly if you run the model again, since I'm not sure if all the randomness has been controlled.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_model_coefficients(\n",
    "    model_object=linear_lasso_model_object,\n",
    "    predictor_names=list(training_predictor_table)\n",
    ")\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression with $L_1$ and $L_2$: Example\n",
    "\n",
    " - The next cell trains a linear-regression model with both penalties.\n",
    " - Whereas $L_1$ only is called the \"lasso penalty\" and $L_2$ only is called the \"ridge penalty,\" this is called the **\"elastic net\"**.\n",
    "<br><br>\n",
    " - In the literature you may see people talk about \"lasso regression,\" \"ridge regression,\" and \"elastic-net regression\".\n",
    " - **These are not real things**.  The lasso, ridge, and elastic-net penalties can be applied to any kind of model.\n",
    " - When people say \"lasso regression,\" \"ridge regression,\" and \"elastic-net regression,\" they generally mean linear regression with the given penalty.\n",
    " - However, this is not always true, so be careful.\n",
    "\n",
    "<br>\n",
    "In this elastic net makes training and validation performance **much worse**, because the values are too high ($\\lambda_1 = 10^{-5}$ and $\\lambda_2 = 5$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_en_model_object = utils.setup_linear_regression(\n",
    "    lambda1=1e-5, lambda2=5.)\n",
    "\n",
    "_ = utils.train_linear_regression(\n",
    "    model_object=linear_en_model_object,\n",
    "    training_predictor_table=training_predictor_table,\n",
    "    training_target_table=training_target_table)\n",
    "\n",
    "training_predictions = linear_en_model_object.predict(\n",
    "    training_predictor_table.as_matrix()\n",
    ")\n",
    "mean_training_target_value = numpy.mean(\n",
    "    training_target_table[utils.TARGET_NAME].values\n",
    ")\n",
    "\n",
    "_ = utils.evaluate_regression(\n",
    "    target_values=training_target_table[utils.TARGET_NAME].values,\n",
    "    predicted_target_values=training_predictions,\n",
    "    mean_training_target_value=mean_training_target_value,\n",
    "    dataset_name='training')\n",
    "print(MINOR_SEPARATOR_STRING)\n",
    "\n",
    "validation_predictions = linear_en_model_object.predict(\n",
    "    validation_predictor_table.as_matrix()\n",
    ")\n",
    "\n",
    "_ = utils.evaluate_regression(\n",
    "    target_values=validation_target_table[utils.TARGET_NAME].values,\n",
    "    predicted_target_values=validation_predictions,\n",
    "    mean_training_target_value=mean_training_target_value,\n",
    "    dataset_name='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression with $L_1$ and $L_2$: Coefficients\n",
    "\n",
    "The next cell plots coefficients for linear regression with both penalties.\n",
    "<br><br>\n",
    "In general the coefficients are an order of magnitude smaller than the original model.  Also, there are not many zero coefficients.\n",
    "<br><br>\n",
    "This means that, with the chosen $\\lambda_1$ and $\\lambda_2$, $L_2$ regularization \"drowned out\" $L_1$ regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_model_coefficients(\n",
    "    model_object=linear_en_model_object,\n",
    "    predictor_names=list(training_predictor_table)\n",
    ")\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# $L_1$ and $L_2$ Regularization: Exercise\n",
    "\n",
    "Train a linear-regression model with your own $\\lambda_1$ and $\\lambda_2$ values.  Investigate performance on the training and validation data and plot coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Experiment with $L_1$ and $L_2$ Regularization\n",
    "\n",
    "The next few cells will show you how to conduct a \"hyperparameter experiment\".  **The steps in a hyperparameter experiment are as follows.**\n",
    "\n",
    " 1. **Choose the values to be attempted.**  This is usually based on some *a priori* knowledge about how the model works.  The more expertise you have, the narrower you can make the range of attempted values.  In this case we attempt $\\lambda_1 \\in \\lbrace 10^{-8}, 10^{-7.5}, 10^{-7}, 10^{-6.5}, 10^{-6}, 10^{-5.5}, 10^{-5}, 10^{-4.5}, 10^{-4} \\rbrace$ and $\\lambda_2 \\in \\lbrace 10^{-4}, 10^{-3.5}, 10^{-3}, 10^{-2.5}, 10^{-2}, 10^{-1.5}, 10^{-1}, 10^{-0.5}, 10^{0}, 10^{0.5}, 10^{1} \\rbrace$.\n",
    " 2. **Train a model with each combination of hyperparameters.**  In this case there are 9 values for $\\lambda_1$ and 11 values for $\\lambda_2$, so 99 combinations.  This is called a \"grid search\".  (Note: There are search methods other than grid search.  These become useful especially when the number of combinations is too large (\"combinatiorial explosion\"), which usually happens if you are experimenting with anymore than a few hyperparameters.  In this case you can do random search or beam search, use a genetic algorithm to evolve the hyperparameters, etc.  But we will stick with grid search in this module.)\n",
    " 3. **Evaluate each model on the validation data.**\n",
    " 4. **Find the model that performs best on the validation data.**  Again, there are many ways to define \"best\".  In this case we will select the model with the highest MAE skill score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Experiment: Training\n",
    "\n",
    "The next cell performs steps 1 and 2 of the hyperparameter experiment (defining the values to be attempted and training the models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda1_values = numpy.logspace(-8, -4, num=9)\n",
    "lambda2_values = numpy.logspace(-4, 1, num=11)\n",
    "\n",
    "num_lambda1 = len(lambda1_values)\n",
    "num_lambda2 = len(lambda2_values)\n",
    "\n",
    "validation_mae_matrix_s01 = numpy.full(\n",
    "    (num_lambda1, num_lambda2), numpy.nan\n",
    ")\n",
    "validation_mse_matrix_s02 = numpy.full(\n",
    "    (num_lambda1, num_lambda2), numpy.nan\n",
    ")\n",
    "validation_mae_skill_matrix = numpy.full(\n",
    "    (num_lambda1, num_lambda2), numpy.nan\n",
    ")\n",
    "validation_mse_skill_matrix = numpy.full(\n",
    "    (num_lambda1, num_lambda2), numpy.nan\n",
    ")\n",
    "\n",
    "mean_training_target_value = numpy.mean(\n",
    "    training_target_table[utils.TARGET_NAME].values\n",
    ")\n",
    "\n",
    "for i in range(num_lambda1):\n",
    "    for j in range(num_lambda2):\n",
    "        this_message_string = (\n",
    "            'Training model with lasso coeff = 10^{0:.1f}, ridge coeff = '\n",
    "            '10^{1:.1f}...'\n",
    "        ).format(\n",
    "            numpy.log10(lambda1_values[i]), numpy.log10(lambda2_values[j])\n",
    "        )\n",
    "\n",
    "        print(this_message_string)\n",
    "\n",
    "        this_model_object = utils.setup_linear_regression(\n",
    "            lambda1=lambda1_values[i], lambda2=lambda2_values[j]\n",
    "        )\n",
    "\n",
    "        _ = utils.train_linear_regression(\n",
    "            model_object=this_model_object,\n",
    "            training_predictor_table=training_predictor_table,\n",
    "            training_target_table=training_target_table)\n",
    "\n",
    "        these_validation_predictions = this_model_object.predict(\n",
    "            validation_predictor_table.as_matrix()\n",
    "        )\n",
    "\n",
    "        this_evaluation_dict = utils.evaluate_regression(\n",
    "            target_values=validation_target_table[utils.TARGET_NAME].values,\n",
    "            predicted_target_values=these_validation_predictions,\n",
    "            mean_training_target_value=mean_training_target_value,\n",
    "            verbose=False, create_plots=False)\n",
    "\n",
    "        validation_mae_matrix_s01[i, j] = this_evaluation_dict[\n",
    "            utils.MAE_KEY]\n",
    "        validation_mse_matrix_s02[i, j] = this_evaluation_dict[\n",
    "            utils.MSE_KEY]\n",
    "        validation_mae_skill_matrix[i, j] = this_evaluation_dict[\n",
    "            utils.MAE_SKILL_SCORE_KEY]\n",
    "        validation_mse_skill_matrix[i, j] = this_evaluation_dict[\n",
    "            utils.MSE_SKILL_SCORE_KEY]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Experiment: Validation\n",
    "\n",
    "The next cell performs step 3 of the hyperparameter experiment (evaluates each model on the validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_scores_2d(\n",
    "    score_matrix=validation_mae_matrix_s01,\n",
    "    min_colour_value=numpy.percentile(validation_mae_matrix_s01, 1.),\n",
    "    max_colour_value=numpy.percentile(validation_mae_matrix_s01, 99.),\n",
    "    x_tick_labels=numpy.log10(lambda2_values),\n",
    "    y_tick_labels=numpy.log10(lambda1_values)\n",
    ")\n",
    "\n",
    "pyplot.xlabel(r'log$_{10}$ of ridge coefficient ($\\lambda_2$)')\n",
    "pyplot.ylabel(r'log$_{10}$ of lasso coefficient ($\\lambda_1$)')\n",
    "pyplot.title(r'Mean absolute error (s$^{-1}$) on validation data')\n",
    "\n",
    "utils.plot_scores_2d(\n",
    "    score_matrix=validation_mse_matrix_s02,\n",
    "    min_colour_value=numpy.percentile(validation_mse_matrix_s02, 1.),\n",
    "    max_colour_value=numpy.percentile(validation_mse_matrix_s02, 99.),\n",
    "    x_tick_labels=numpy.log10(lambda2_values),\n",
    "    y_tick_labels=numpy.log10(lambda1_values)\n",
    ")\n",
    "\n",
    "pyplot.xlabel(r'log$_{10}$ of ridge coefficient ($\\lambda_2$)')\n",
    "pyplot.ylabel(r'log$_{10}$ of lasso coefficient ($\\lambda_1$)')\n",
    "pyplot.title(r'Mean squared error (s$^{-2}$) on validation data')\n",
    "\n",
    "utils.plot_scores_2d(\n",
    "    score_matrix=validation_mae_skill_matrix,\n",
    "    min_colour_value=numpy.percentile(validation_mae_skill_matrix, 1.),\n",
    "    max_colour_value=numpy.percentile(validation_mae_skill_matrix, 99.),\n",
    "    x_tick_labels=numpy.log10(lambda2_values),\n",
    "    y_tick_labels=numpy.log10(lambda1_values)\n",
    ")\n",
    "\n",
    "pyplot.xlabel(r'log$_{10}$ of ridge coefficient ($\\lambda_2$)')\n",
    "pyplot.ylabel(r'log$_{10}$ of lasso coefficient ($\\lambda_1$)')\n",
    "pyplot.title(r'MAE skill score on validation data')\n",
    "\n",
    "utils.plot_scores_2d(\n",
    "    score_matrix=validation_mse_skill_matrix,\n",
    "    min_colour_value=numpy.percentile(validation_mse_skill_matrix, 1.),\n",
    "    max_colour_value=numpy.percentile(validation_mse_skill_matrix, 99.),\n",
    "    x_tick_labels=numpy.log10(lambda2_values),\n",
    "    y_tick_labels=numpy.log10(lambda1_values)\n",
    ")\n",
    "\n",
    "pyplot.xlabel(r'log$_{10}$ of ridge coefficient ($\\lambda_2$)')\n",
    "pyplot.ylabel(r'log$_{10}$ of lasso coefficient ($\\lambda_1$)')\n",
    "pyplot.title(r'MSE skill score on validation data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Experiment: Selection\n",
    "\n",
    "The next cell performs step 4 of the hyperparameter experiment (select model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_linear_index = numpy.argmax(numpy.ravel(validation_mae_skill_matrix))\n",
    "\n",
    "best_lambda1_index, best_lambda2_index = numpy.unravel_index(\n",
    "    best_linear_index, (len(lambda1_values), len(lambda2_values))\n",
    ")\n",
    "\n",
    "best_lambda1 = lambda1_values[best_lambda1_index]\n",
    "best_lambda2 = lambda2_values[best_lambda2_index]\n",
    "best_validation_maess = numpy.max(validation_mae_skill_matrix)\n",
    "\n",
    "message_string = (\n",
    "    'Best MAE skill score on validation data = {0:.3f} ... corresponding '\n",
    "    'lasso coeff = 10^{1:.1f}, ridge coeff = 10^{2:.1f}'\n",
    ").format(\n",
    "    best_validation_maess, numpy.log10(best_lambda1),\n",
    "    numpy.log10(best_lambda2)\n",
    ")\n",
    "\n",
    "print(message_string)\n",
    "\n",
    "final_model_object = utils.setup_linear_regression(\n",
    "    lambda1=best_lambda1, lambda2=best_lambda2)\n",
    "\n",
    "_ = utils.train_linear_regression(\n",
    "    model_object=final_model_object,\n",
    "    training_predictor_table=training_predictor_table,\n",
    "    training_target_table=training_target_table)\n",
    "\n",
    "testing_predictions = final_model_object.predict(\n",
    "    testing_predictor_table.as_matrix()\n",
    ")\n",
    "mean_training_target_value = numpy.mean(\n",
    "    training_target_table[utils.TARGET_NAME].values\n",
    ")\n",
    "\n",
    "this_evaluation_dict = utils.evaluate_regression(\n",
    "    target_values=testing_target_table[utils.TARGET_NAME].values,\n",
    "    predicted_target_values=testing_predictions,\n",
    "    mean_training_target_value=mean_training_target_value,\n",
    "    dataset_name='testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Experiment: Exercise\n",
    "\n",
    "Based on the first hyperparameter experiment, write your own hyperparameter experiment (with a different set of $\\lambda_1$ and $\\lambda_2$ values).  See if you can find a $\\lambda_1$-$\\lambda_2$ combo that works better on the validation data.  If you find a combo that works better on the validation data, see if it also works better on the testing data.  If not, you have overfit $\\lambda_1$ and $\\lambda_2$ to the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Classification\n",
    "\n",
    " - **The rest of this module focuses on binary classification, rather than regression.**\n",
    " - \"Regression\" is the prediction of a real number (*e.g.*, above, where we predicted max future vorticity).\n",
    " - \"Classification\" is the prediction of a category (*e.g.*, low, medium, or high max future vorticity).\n",
    "<br><br>\n",
    " - **In binary classification there are two categories.**\n",
    " - Thus, prediction takes the form of answering a **yes-or-no question.**\n",
    " - We will use the same target variable (max future vorticity), except we will binarize it.\n",
    " - The problem will be predicting whether or not max future vorticity exceeds a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarization\n",
    "\n",
    " - The next cell \"binarizes\" the target variable (turns each value into a 0 or 1, yes or no).\n",
    " - The threshold is the 90$^{\\textrm{th}}$ percentile of max future vorticity over all training examples.\n",
    " - The same threshold is used to binarize training, validation, and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarization_threshold = utils.get_binarization_threshold(\n",
    "    csv_file_names=training_file_names, percentile_level=90.)\n",
    "print(MINOR_SEPARATOR_STRING)\n",
    "\n",
    "these_target_values = (\n",
    "    training_target_table[utils.TARGET_NAME].values[:10]\n",
    ")\n",
    "\n",
    "message_string = (\n",
    "    'Real-numbered target values for the first training examples:\\n{0:s}'\n",
    ").format(str(these_target_values))\n",
    "print(message_string)\n",
    "\n",
    "training_target_values = utils.binarize_target_values(\n",
    "    target_values=training_target_table[utils.TARGET_NAME].values,\n",
    "    binarization_threshold=binarization_threshold)\n",
    "\n",
    "training_target_table = training_target_table.assign(\n",
    "    **{utils.BINARIZED_TARGET_NAME: training_target_values}\n",
    ")\n",
    "\n",
    "print('\\nBinarization threshold = {0:.3e} s^-1'.format(\n",
    "    binarization_threshold\n",
    "))\n",
    "\n",
    "these_target_values = (\n",
    "    training_target_table[utils.TARGET_NAME].values[:10]\n",
    ")\n",
    "\n",
    "message_string = (\n",
    "    '\\nBinarized target values for the first training examples:\\n{0:s}'\n",
    ").format(str(these_target_values))\n",
    "print(message_string)\n",
    "\n",
    "validation_target_values = utils.binarize_target_values(\n",
    "    target_values=validation_target_table[utils.TARGET_NAME].values,\n",
    "    binarization_threshold=binarization_threshold)\n",
    "\n",
    "validation_target_table = validation_target_table.assign(\n",
    "    **{utils.BINARIZED_TARGET_NAME: validation_target_values}\n",
    ")\n",
    "\n",
    "testing_target_values = utils.binarize_target_values(\n",
    "    target_values=testing_target_table[utils.TARGET_NAME].values,\n",
    "    binarization_threshold=binarization_threshold)\n",
    "\n",
    "testing_target_table = testing_target_table.assign(\n",
    "    **{utils.BINARIZED_TARGET_NAME: testing_target_values}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarization: Exercise\n",
    "\n",
    "If you want to use another percentile (instead of the 90$^{\\textrm{th}}$) for binarization, feel free to do so here.  For example, if you want to work with a non-rare event, you could use something like the 50$^{\\textrm{th}}$ percentile.  If you want to work with an even rarer event, you could use the 99$^{\\textrm{th}}$ percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contingency Table\n",
    "\n",
    "**Most evaluation methods for binary classification are based on the contingency table** (or \"confusion matrix\"), shown below.\n",
    "\n",
    "<img src=\"contingency_table.png\" alt=\"Contingency table\" width=\"500\" />\n",
    "\n",
    " - $a$ = number of true positives (forecast = label = \"yes\")\n",
    " - $b$ = number of false positives (forecast = \"yes\" but label = \"no\")\n",
    " - $c$ = number of false negatives (forecast = \"no\" but label = \"yes\")\n",
    " - $d$ = number of true negatives (forecast = label = \"no\")\n",
    "<br><br>\n",
    " - **Problem**: most classification models (including logistic regression) output probabilities, rather than yeses and nos.\n",
    " - **Solution**: determinize the probabilities.\n",
    " - \"Determinize\" is just a fancy way of saying \"turn the probabilities into yeses and nos\".\n",
    " - This is done with the following equation, where $p$ = probability; $p^*$ = threshold; and $\\hat{y}$ is the resulting deterministic forecast.\n",
    "\n",
    "$\\hat{y} = \\begin{cases}1,\\quad\\textrm{if }p \\ge p^* \\\\0,\\quad\\textrm{otherwise}\\end{cases}$\n",
    "\n",
    " - The best threshold is usually **not** 0.5.\n",
    " - Keep in mind that $p^*$ is a hyperparameter, so it should be optimized on the validation data.\n",
    "\n",
    "**The following scores can be computed from the contingency table.**\n",
    "\n",
    "<img src=\"ct_scores.png\" alt=\"Scores from contingency table\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "\n",
    " - The ROC curve plots POD (probability of detection) vs. POFD (probability of false detection) with varying threshold $p^*$.\n",
    " - Each point in the curve corresponds to one $p^*$ value.\n",
    " - For $p^*$ = 0, POD = POFD = 1.\n",
    " - For $p^*$ = 1, POD = POFD = 0.\n",
    "\n",
    "**Two numbers can be used to summarize the goodness of the ROC curve:**\n",
    "\n",
    " - Maximum Peirce score (POD - POFD) over all thresholds\n",
    " - Area under the curve (AUC)\n",
    " - Generally, an AUC $\\ge$ 0.9 is considered \"excellent\".\n",
    " - AUC = 0.5 (dashed grey line) for a random model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Diagram\n",
    "\n",
    " - The performance diagram plots POD (probability of detection) vs. success ratio (1 - FAR).\n",
    " - Again, each point corresponds to one $p^*$ value.\n",
    " - For $p^*$ = 0, POD = 1 and success ratio is minimized.\n",
    " - For $p^*$ = 1, POD = 0 and success ratio is maximized.\n",
    "\n",
    "CSI (critical success index) and frequency bias are contoured in the background.  They can be expressed in POD&ndash;success-ratio space as follows.\n",
    "<br><br>\n",
    "$\\textrm{bias} = \\frac{\\textrm{POD}}{\\textrm{SR}}$\n",
    "<br>\n",
    "$\\textrm{CSI}^{-1} = \\textrm{POD}^{-1} + \\textrm{SR}^{-1} - 1$\n",
    "<br><br>\n",
    "\n",
    "**A few numbers can be used to summarize the goodness of the performance diagram:**\n",
    "\n",
    " - Maximum CSI\n",
    " - Frequency bias at maximum CSI (should occur with a frequency bias near 1.0)\n",
    " - Area under curve\n",
    "<br><br>\n",
    " - However, unlike the ROC curve, these numbers are very sensitive to the class distribution.\n",
    " - Thus, thresholds for \"good\" and \"bad\" depend on the class distribution.\n",
    " - For some rare events a max CSI of 0.10 may be excellent; for common events a max CSI of 0.8 may be poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes Diagram\n",
    "\n",
    " - The attributes diagram is a reliability curve for binary classification, plus a few reference lines in the background.\n",
    " - **Plots forecast probability vs. conditional mean frequency.**\n",
    " - This answers the question: **\"given forecast probability $p$, how likely is the event?\"**\n",
    "<br><br>\n",
    " - **For perfect reliability, conditional event frequency = forecast probability always.**\n",
    " - This is represented by the diagonal grey line ($x = y$), called the \"perfect-reliability line\".\n",
    " - The vertical grey line is the \"climatology line,\" representing the event frequency in the training data, which is just called \"climatology\".  Climatology in our case is 10%.  If the model always forecast climatology, the reliability curve would collapse to a single point on this line.\n",
    " - The horizontal grey line is the \"no-resolution line,\" also based on training-set climatology.  If the model were completely random, the reliability curve would approx follow this line.\n",
    "<br><br>\n",
    " - **The blue shading is the \"positive-skill area,\" where Brier skill score $>$ 0.**\n",
    " - Brier skill score (BSS) is the Brier score (BS) relative to climatology.\n",
    "<br><br>\n",
    "$\\textrm{BS} = \\frac{1}{N} \\sum\\limits_{i = 1}^{N} (p_i - y_i)^2$\n",
    "<br>\n",
    "$\\textrm{BSS} = \\frac{\\textrm{BS}_{\\textrm{climo}} - \\textrm{BS}}{\\textrm{BS}_{\\textrm{climo}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Experiment with Minimum Sample Size\n",
    "\n",
    " - **Two hyperparameters (among others) control the depth of a decision tree:** minimum examples per branch node ($N_b^{\\textrm{min}}$) and per leaf node ($N_l^{\\textrm{min}}$).\n",
    " - If these values are set to 1, the tree can become very deep, which increases its ability to overfit.\n",
    " - You can think of this another way: if there is one example at each leaf node, all predictions will be based on only one example and will probably not generalize well to new data.\n",
    "<br><br>\n",
    " - Conversely, if $N_b^{\\textrm{min}}$ and $N_l^{\\textrm{min}}$ are set too high, the tree will not become deep enough, causing it to underfit.\n",
    " - For example, suppose that you have 1000 training examples and set $N_l^{\\textrm{min}}$ to 1000.\n",
    " - This will allow only one branch node (the root node); both children of the root node will have $<$ 1000 examples.\n",
    " - Thus, predictions will be based on only one question.\n",
    "\n",
    "<br>\n",
    "**Recall the four steps of any hyperparameter experiment:**\n",
    "\n",
    " 1. Choose the values to be attempted.  We will try $N_b^{\\textrm{min}} \\in \\lbrace 2, 5, 10, 20, 30, 40, 50, 100, 200, 500 \\rbrace$ and $N_l^{\\textrm{min}} \\in \\lbrace 1, 5, 10, 20, 30, 40, 50, 100, 200, 500 \\rbrace$.  However, we will not try combinations where $N_l^{\\textrm{min}} \\ge N_b^{\\textrm{min}}$, because this makes no sense (the child of a node with $N$ examples cannot have $\\ge N$ examples).\n",
    " 2. Train a model with each combination.\n",
    " 3. Evaluate each model on the validation data.\n",
    " 4. Select the model that performs best on validation data.  Here we will define \"best\" as that with the highest Brier skill score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Experiment: Training\n",
    "\n",
    "The next cell performs steps 1 and 2 of the hyperparameter experiment (defining the values to be attempted and training the models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_per_split_values = numpy.array(\n",
    "    [2, 5, 10, 20, 30, 40, 50, 100, 200, 500], dtype=int)\n",
    "min_per_leaf_values = numpy.array(\n",
    "    [1, 5, 10, 20, 30, 40, 50, 100, 200, 500], dtype=int)\n",
    "\n",
    "num_min_per_split_values = len(min_per_split_values)\n",
    "num_min_per_leaf_values = len(min_per_leaf_values)\n",
    "\n",
    "validation_auc_matrix = numpy.full(\n",
    "    (num_min_per_split_values, num_min_per_leaf_values), numpy.nan\n",
    ")\n",
    "\n",
    "validation_max_csi_matrix = validation_auc_matrix + 0.\n",
    "validation_bs_matrix = validation_auc_matrix + 0.\n",
    "validation_bss_matrix = validation_auc_matrix + 0.\n",
    "\n",
    "training_event_frequency = numpy.mean(\n",
    "    training_target_table[utils.BINARIZED_TARGET_NAME].values\n",
    ")\n",
    "\n",
    "for i in range(num_min_per_split_values):\n",
    "    for j in range(num_min_per_leaf_values):\n",
    "        if min_per_leaf_values[j] >= min_per_split_values[i]:\n",
    "            continue\n",
    "\n",
    "        this_message_string = (\n",
    "            'Training model with minima of {0:d} examples per split node, '\n",
    "            '{1:d} per leaf node...'\n",
    "        ).format(min_per_split_values[i], min_per_leaf_values[j])\n",
    "\n",
    "        print(this_message_string)\n",
    "\n",
    "        this_model_object = utils.setup_classification_tree(\n",
    "            min_examples_at_split=min_per_split_values[i],\n",
    "            min_examples_at_leaf=min_per_leaf_values[j]\n",
    "        )\n",
    "\n",
    "        _ = utils.train_classification_tree(\n",
    "            model_object=this_model_object,\n",
    "            training_predictor_table=training_predictor_table,\n",
    "            training_target_table=training_target_table)\n",
    "\n",
    "        these_validation_predictions = this_model_object.predict_proba(\n",
    "            validation_predictor_table.as_matrix()\n",
    "        )[:, 1]\n",
    "\n",
    "        this_evaluation_dict = utils.eval_binary_classifn(\n",
    "            observed_labels=validation_target_table[\n",
    "                utils.BINARIZED_TARGET_NAME].values,\n",
    "            forecast_probabilities=these_validation_predictions,\n",
    "            training_event_frequency=training_event_frequency,\n",
    "            create_plots=False, verbose=False)\n",
    "\n",
    "        validation_auc_matrix[i, j] = this_evaluation_dict[utils.AUC_KEY]\n",
    "        validation_max_csi_matrix[i, j] = this_evaluation_dict[\n",
    "            utils.MAX_CSI_KEY]\n",
    "        validation_bs_matrix[i, j] = this_evaluation_dict[\n",
    "            utils.BRIER_SCORE_KEY]\n",
    "        validation_bss_matrix[i, j] = this_evaluation_dict[\n",
    "            utils.BRIER_SKILL_SCORE_KEY]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Experiment: Validation\n",
    "\n",
    "The next cell performs step 3 of the hyperparameter experiment (evaluates each model on the validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_scores_2d(\n",
    "    score_matrix=validation_auc_matrix,\n",
    "    min_colour_value=numpy.nanpercentile(validation_auc_matrix, 1.),\n",
    "    max_colour_value=numpy.nanpercentile(validation_auc_matrix, 99.),\n",
    "    x_tick_labels=min_per_leaf_values,\n",
    "    y_tick_labels=min_per_split_values\n",
    ")\n",
    "\n",
    "pyplot.xlabel('Min num examples at leaf node')\n",
    "pyplot.ylabel('Min num examples at split node')\n",
    "pyplot.title('AUC (area under ROC curve) on validation data')\n",
    "\n",
    "utils.plot_scores_2d(\n",
    "    score_matrix=validation_max_csi_matrix,\n",
    "    min_colour_value=numpy.nanpercentile(validation_max_csi_matrix, 1.),\n",
    "    max_colour_value=numpy.nanpercentile(validation_max_csi_matrix, 99.),\n",
    "    x_tick_labels=min_per_leaf_values,\n",
    "    y_tick_labels=min_per_split_values\n",
    ")\n",
    "\n",
    "pyplot.xlabel('Min num examples at leaf node')\n",
    "pyplot.ylabel('Min num examples at split node')\n",
    "pyplot.title('Max CSI (critical success index) on validation data')\n",
    "\n",
    "utils.plot_scores_2d(\n",
    "    score_matrix=validation_bs_matrix,\n",
    "    min_colour_value=numpy.nanpercentile(validation_bs_matrix, 1.),\n",
    "    max_colour_value=numpy.nanpercentile(validation_bs_matrix, 99.),\n",
    "    x_tick_labels=min_per_leaf_values,\n",
    "    y_tick_labels=min_per_split_values\n",
    ")\n",
    "\n",
    "pyplot.xlabel('Min num examples at leaf node')\n",
    "pyplot.ylabel('Min num examples at split node')\n",
    "pyplot.title('Brier score on validation data')\n",
    "\n",
    "utils.plot_scores_2d(\n",
    "    score_matrix=validation_bss_matrix,\n",
    "    min_colour_value=numpy.nanpercentile(validation_bss_matrix, 1.),\n",
    "    max_colour_value=numpy.nanpercentile(validation_bss_matrix, 99.),\n",
    "    x_tick_labels=min_per_leaf_values,\n",
    "    y_tick_labels=min_per_split_values\n",
    ")\n",
    "\n",
    "pyplot.xlabel('Min num examples at leaf node')\n",
    "pyplot.ylabel('Min num examples at split node')\n",
    "pyplot.title('Brier skill score on validation data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Experiment: Selection\n",
    "\n",
    "The next cell performs step 4 of the hyperparameter experiment (select model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_linear_index = numpy.nanargmax(numpy.ravel(validation_bss_matrix))\n",
    "\n",
    "best_split_index, best_leaf_index = numpy.unravel_index(\n",
    "    best_linear_index, validation_bss_matrix.shape)\n",
    "\n",
    "best_min_examples_per_split = min_per_split_values[best_split_index]\n",
    "best_min_examples_per_leaf = min_per_leaf_values[best_leaf_index]\n",
    "best_validation_bss = numpy.nanmax(validation_bss_matrix)\n",
    "\n",
    "message_string = (\n",
    "    'Best validation BSS = {0:.3f} ... corresponding min examples per split'\n",
    "    ' node = {1:d} ... min examples per leaf node = {2:d}'\n",
    ").format(\n",
    "    best_validation_bss, best_min_examples_per_split,\n",
    "    best_min_examples_per_leaf\n",
    ")\n",
    "\n",
    "print(message_string)\n",
    "\n",
    "final_model_object = utils.setup_classification_tree(\n",
    "    min_examples_at_split=best_min_examples_per_split,\n",
    "    min_examples_at_leaf=best_min_examples_per_leaf\n",
    ")\n",
    "\n",
    "_ = utils.train_classification_tree(\n",
    "    model_object=final_model_object,\n",
    "    training_predictor_table=training_predictor_table,\n",
    "    training_target_table=training_target_table)\n",
    "\n",
    "testing_predictions = final_model_object.predict_proba(\n",
    "    testing_predictor_table.as_matrix()\n",
    ")[:, 1]\n",
    "training_event_frequency = numpy.mean(\n",
    "    training_target_table[utils.BINARIZED_TARGET_NAME].values\n",
    ")\n",
    "\n",
    "_ = utils.eval_binary_classifn(\n",
    "    observed_labels=testing_target_table[\n",
    "        utils.BINARIZED_TARGET_NAME].values,\n",
    "    forecast_probabilities=testing_predictions,\n",
    "    training_event_frequency=training_event_frequency,\n",
    "    create_plots=True, verbose=True, dataset_name='testing')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}